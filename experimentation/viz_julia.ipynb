{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on Python <> Julia integration\n",
    "The below script relies on Matplotlib and PyTorch being available in the Conda environment attached to Julia. If you don't tend to use Julia very much, you may want to attach one of your favourite Python environments to Julia to avoid reinstalling a bunch of stuff. To do this, run the following commands with the appropriate Python executable referenced. (If PyCall is not yet installed, you could use `pkg\"add PyCall\"` instead of `build`.\n",
    "\n",
    "```julia\n",
    "ENV[\"PYTHON\"]=expanduser(\"~/anaconda3/envs/mkl/bin/python\")  # change path here\n",
    "using Pkg\n",
    "pkg\"build PyCall\"\n",
    "```\n",
    "Thanks to [Przemyslaw Szufel](https://stackoverflow.com/a/63182917) for this notebook friendly route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, StatsBase, Random\n",
    "using PyPlot\n",
    "using BSON, NPZ, CSV, Tables\n",
    "\n",
    "using MeshCat\n",
    "using Quaternions, GeometryBasics, CoordinateTransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.mocaputil"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project-specific Julia code\n",
    "include(\"viz/mocap_viz.jl\")\n",
    "include(\"viz/expmtdata.jl\")\n",
    "include(\"viz/geom3d.jl\")\n",
    "include(\"viz/prettytbl.jl\")\n",
    "include(\"viz/util.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Recall that the 30fps processed data can be downloaded from [https://bit.ly/38x2sra](https://bit.ly/38x2sra). Once downloaded, please store the `.npz` files in the `data/` directory in the base project (parent) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Vector{Int64}}:\n",
       " [1, 2, 3, 4, 5, 6]\n",
       " [7, 8, 9, 10]\n",
       " [11, 12, 13]\n",
       " [14, 15, 16, 17]\n",
       " [18, 19, 20, 21]\n",
       " [22, 23, 24, 25]\n",
       " [26, 27, 28]\n",
       " [29, 30, 31]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# database = \"../../mocap-mtds-macbook/data/edin-style-transfer/\"\n",
    "# files_edin = [joinpath(database, f) for f in readdir(database)];\n",
    "# style_name_edin = [x[1] for x in match.(r\"[a-z\\-]+/[a-z\\-]+/([a-z]+)_.*\", files_edin)];\n",
    "# styles = unique(style_name_edin)\n",
    "# styles_lkp = [findall(s .== style_name_edin) for s in styles]\n",
    "styles_lkp = [\n",
    "    [1, 2, 3, 4, 5, 6],\n",
    "    [7, 8, 9, 10],\n",
    "    [11, 12, 13],\n",
    "    [14, 15, 16, 17],\n",
    "    [18, 19, 20, 21],\n",
    "    [22, 23, 24, 25],\n",
    "    [26, 27, 28],\n",
    "    [29, 30, 31]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not perform the following cell.**\n",
    "\n",
    "We need to invert the standardization performed to create the data files used to train/test the model. This\n",
    "has been saved in BSON format, but the original code to do this is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in data\n",
    "# data_path2 = \"../../mocap-mtds-macbook/\"\n",
    "# Usraw1 = BSON.load(joinpath(data_path2, \"edin_Xs_30fps_final.bson\"))[:Xs];\n",
    "# Ysraw1 = BSON.load(joinpath(data_path2, \"edin_Ys_30fps_final.bson\"))[:Ys];\n",
    "\n",
    "# standardize_Y1 = fit(mocaputil.MyStandardScaler, reduce(vcat, Ysraw1),  1)\n",
    "# standardize_U1 = fit(mocaputil.MyStandardScaler, reduce(vcat, Usraw1),  1)\n",
    "\n",
    "# BSON.bson(\"../data/standardization_Y.bson\", Dict(\n",
    "#     \"μ\"=>standardize_Y1.μ, \n",
    "#     \"σ\"=>standardize_Y1.σ, \n",
    "#     \"operate_on\"=>standardize_Y1.operate_on,\n",
    "#     \"dims\"=>standardize_Y1.dims\n",
    "# ))\n",
    "# BSON.bson(\"../data/standardization_U.bson\", Dict(\n",
    "#     \"μ\"=>standardize_U1.μ, \n",
    "#     \"σ\"=>standardize_U1.σ,\n",
    "#     \"operate_on\"=>standardize_U1.operate_on,\n",
    "#     \"dims\"=>standardize_U1.dims\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in data\n",
    "data_path = \"../data/\"\n",
    "\n",
    "#= Load from Numpy Compressed files dumped from python pre-processing =#\n",
    "Ys = npzread(joinpath(data_path, \"edin_Ys_30fps_final.npz\"))\n",
    "Us = npzread(joinpath(data_path, \"edin_Us_30fps_final.npz\"))\n",
    "\n",
    "# string-based dict to standard (int-based) array\n",
    "Ys = [Ys[string(i)] for i in 1:31]\n",
    "Us = [Us[string(i)] for i in 1:31];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_Y = mocaputil.MyStandardScaler(\n",
    "    let d=BSON.load(\"../data/standardization_Y.bson\"); d[\"μ\"], d[\"σ\"], d[\"operate_on\"], d[\"dims\"]; end...\n",
    ")\n",
    "standardize_U = mocaputil.MyStandardScaler(\n",
    "    let d=BSON.load(\"../data/standardization_U.bson\"); d[\"μ\"], d[\"σ\"], d[\"operate_on\"], d[\"dims\"]; end...\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_expmtdata = expmtdata.ExperimentData(\n",
    "    [zeros(Float32, size(y,1)+1, size(y,2)) for y in Ys],    # Ysraw -- legacy, not needed\n",
    "    [Matrix(y') for y in Ys], \n",
    "    [Matrix(u') for u in Us], \n",
    "    styles_lkp\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Training / test data for models (see note below)\n",
    "**This section is not specifially useful here**; but this is for demonstration purposes. In my experimental work I saved\n",
    "these batched training/test sets as `.npz` files and used them to feed to the PyTorch model in a training shell script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "get_data(s::ExperimentData, ix, splittype, tasktype)\n",
       "\\end{verbatim}\n",
       "Convenience utility for accessing data stored in an ExperimentData struct. Specify the index of the target task, and then select from:\n",
       "\n",
       "splittype:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{:all}        - return the concatentation of all training/validation/test data.\n",
       "\n",
       "\n",
       "\\item \\textbf{:trainvalid} - return the concatentation of all training/validation data.\n",
       "\n",
       "\n",
       "\\item \\textbf{:split}      - return individual (3x) outputs for training/validation/test data.\n",
       "\n",
       "\n",
       "\\item \\textbf{:test}       - return only the test data\n",
       "\n",
       "\n",
       "\\item \\textbf{:train}      - return only the train data\n",
       "\n",
       "\n",
       "\\item \\textbf{:valid}      - return only the validation data.\n",
       "\n",
       "\\end{itemize}\n",
       "tasktype:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{:stl}   - single task model. Return train/validation/test data from this task's data.\n",
       "\n",
       "\n",
       "\\item \\textbf{:pool}  - pooled/cohort model. Here, training and validation data are from the        complement of the selected index, returned in individual wrappers.\n",
       "\n",
       "\\end{itemize}\n",
       "Note that in all cases, the output will be (a) Dict(s) containing the following fields:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{:Y}    - the observation matrix (each column is an observation).\n",
       "\n",
       "\n",
       "\\item \\textbf{:U}    - the input matrix (each column is a datapoint).\n",
       "\n",
       "\n",
       "\\item \\textbf{:Yraw} - the raw data before standardisation and another manipulation. (Possibly legacy?)\n",
       "\n",
       "\\end{itemize}\n",
       "Othe kwargs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{concat}  - By default, each boundary encountered between files will result in\n",
       "\n",
       "\\end{itemize}\n",
       "a separate Dict, so the return values will be a vector of Dicts. However, for more basic models (such as linear regression) with no assumption of temporal continuity, it may be simpler to operate on a standard input and output data matrix. Setting \\texttt{concat = true} will return just a single Dict in an array with all the data. Choosing \\texttt{simplify=true} will further remove the array, returning only Dicts.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{stratified}  - (STL only) stratify the validation/test sets across files in\n",
       "\n",
       "\\end{itemize}\n",
       "each style. By default, the test set will come at the end of the concatenation of all files. Stratifying will mean there are L test sets from each of L files. For the pooled dataset, the test set is partially stratified, that is, it is stratified over the \\emph{types} (i.e. a \\% of each style), but not over the \\emph{files} within the types. Given that our goal is MTL, this seems appropriate.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{split}  - The train/validation/test split as a simplicial 3-dim vector.\n",
       "\n",
       "\n",
       "\\item \\texttt{simplify} - See \\texttt{concat}. Used without \\texttt{concat} this option does nothing.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "get_data(s::ExperimentData, ix, splittype, tasktype)\n",
       "```\n",
       "\n",
       "Convenience utility for accessing data stored in an ExperimentData struct. Specify the index of the target task, and then select from:\n",
       "\n",
       "splittype:\n",
       "\n",
       "  * **:all**        - return the concatentation of all training/validation/test data.\n",
       "  * **:trainvalid** - return the concatentation of all training/validation data.\n",
       "  * **:split**      - return individual (3x) outputs for training/validation/test data.\n",
       "  * **:test**       - return only the test data\n",
       "  * **:train**      - return only the train data\n",
       "  * **:valid**      - return only the validation data.\n",
       "\n",
       "tasktype:\n",
       "\n",
       "  * **:stl**   - single task model. Return train/validation/test data from this task's data.\n",
       "  * **:pool**  - pooled/cohort model. Here, training and validation data are from the        complement of the selected index, returned in individual wrappers.\n",
       "\n",
       "Note that in all cases, the output will be (a) Dict(s) containing the following fields:\n",
       "\n",
       "  * **:Y**    - the observation matrix (each column is an observation).\n",
       "  * **:U**    - the input matrix (each column is a datapoint).\n",
       "  * **:Yraw** - the raw data before standardisation and another manipulation. (Possibly legacy?)\n",
       "\n",
       "Othe kwargs:\n",
       "\n",
       "  * `concat`  - By default, each boundary encountered between files will result in\n",
       "\n",
       "a separate Dict, so the return values will be a vector of Dicts. However, for more basic models (such as linear regression) with no assumption of temporal continuity, it may be simpler to operate on a standard input and output data matrix. Setting `concat = true` will return just a single Dict in an array with all the data. Choosing `simplify=true` will further remove the array, returning only Dicts.\n",
       "\n",
       "  * `stratified`  - (STL only) stratify the validation/test sets across files in\n",
       "\n",
       "each style. By default, the test set will come at the end of the concatenation of all files. Stratifying will mean there are L test sets from each of L files. For the pooled dataset, the test set is partially stratified, that is, it is stratified over the *types* (i.e. a % of each style), but not over the *files* within the types. Given that our goal is MTL, this seems appropriate.\n",
       "\n",
       "  * `split`  - The train/validation/test split as a simplicial 3-dim vector.\n",
       "  * `simplify` - See `concat`. Used without `concat` this option does nothing.\n"
      ],
      "text/plain": [
       "\u001b[36m  get_data(s::ExperimentData, ix, splittype, tasktype)\u001b[39m\n",
       "\n",
       "  Convenience utility for accessing data stored in an ExperimentData struct.\n",
       "  Specify the index of the target task, and then select from:\n",
       "\n",
       "  splittype:\n",
       "\n",
       "    •  \u001b[1m:all\u001b[22m - return the concatentation of all training/validation/test data.\n",
       "\n",
       "    •  \u001b[1m:trainvalid\u001b[22m - return the concatentation of all training/validation\n",
       "       data.\n",
       "\n",
       "    •  \u001b[1m:split\u001b[22m - return individual (3x) outputs for training/validation/test\n",
       "       data.\n",
       "\n",
       "    •  \u001b[1m:test\u001b[22m - return only the test data\n",
       "\n",
       "    •  \u001b[1m:train\u001b[22m - return only the train data\n",
       "\n",
       "    •  \u001b[1m:valid\u001b[22m - return only the validation data.\n",
       "\n",
       "  tasktype:\n",
       "\n",
       "    •  \u001b[1m:stl\u001b[22m - single task model. Return train/validation/test data from this\n",
       "       task's data.\n",
       "\n",
       "    •  \u001b[1m:pool\u001b[22m - pooled/cohort model. Here, training and validation data are\n",
       "       from the complement of the selected index, returned in individual\n",
       "       wrappers.\n",
       "\n",
       "  Note that in all cases, the output will be (a) Dict(s) containing the\n",
       "  following fields:\n",
       "\n",
       "    •  \u001b[1m:Y\u001b[22m - the observation matrix (each column is an observation).\n",
       "\n",
       "    •  \u001b[1m:U\u001b[22m - the input matrix (each column is a datapoint).\n",
       "\n",
       "    •  \u001b[1m:Yraw\u001b[22m - the raw data before standardisation and another manipulation.\n",
       "       (Possibly legacy?)\n",
       "\n",
       "  Othe kwargs:\n",
       "\n",
       "    •  \u001b[36mconcat\u001b[39m - By default, each boundary encountered between files will\n",
       "       result in\n",
       "\n",
       "  a separate Dict, so the return values will be a vector of Dicts. However,\n",
       "  for more basic models (such as linear regression) with no assumption of\n",
       "  temporal continuity, it may be simpler to operate on a standard input and\n",
       "  output data matrix. Setting \u001b[36mconcat = true\u001b[39m will return just a single Dict in\n",
       "  an array with all the data. Choosing \u001b[36msimplify=true\u001b[39m will further remove the\n",
       "  array, returning only Dicts.\n",
       "\n",
       "    •  \u001b[36mstratified\u001b[39m - (STL only) stratify the validation/test sets across files\n",
       "       in\n",
       "\n",
       "  each style. By default, the test set will come at the end of the\n",
       "  concatenation of all files. Stratifying will mean there are L test sets from\n",
       "  each of L files. For the pooled dataset, the test set is partially\n",
       "  stratified, that is, it is stratified over the \u001b[4mtypes\u001b[24m (i.e. a % of each\n",
       "  style), but not over the \u001b[4mfiles\u001b[24m within the types. Given that our goal is MTL,\n",
       "  this seems appropriate.\n",
       "\n",
       "    •  \u001b[36msplit\u001b[39m - The train/validation/test split as a simplicial 3-dim vector.\n",
       "\n",
       "    •  \u001b[36msimplify\u001b[39m - See \u001b[36mconcat\u001b[39m. Used without \u001b[36mconcat\u001b[39m this option does nothing."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?expmtdata.get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training set for STL and pooled models.\n",
    "style_ix = 1                         # which style is to be held out for test data\n",
    "train_ixs = setdiff(1:8, style_ix) \n",
    "min_size = 63;\n",
    "batch_size = 64;\n",
    "\n",
    "trainPool, validPool, testPool = expmtdata.get_data(this_expmtdata, style_ix, :split, :pooled)\n",
    "trainIter = mocaputil.DataIterator(trainPool, 64, min_size=min_size);\n",
    "trainIters = collect(trainIter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIter = mocaputil.DataIterator(testPool, 64, min_size=min_size);\n",
    "testIters = collect(testIter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this training data look like?\n",
    "Each batch is defined here to have length 64, and hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch b typically has Y_b of size (n_y, T): (67, 64)\n",
      "                           U_b of size (n_u, T): (35, 64)\n",
      "  and an indicator of whether we have transitioned to a new sequence since the last batch (bool)\n"
     ]
    }
   ],
   "source": [
    "println(\"Each batch b typically has Y_b of size (n_y, T): $(size(trainIters[1][1]))\")\n",
    "println(\"                           U_b of size (n_u, T): $(size(trainIters[1][2]))\")\n",
    "println(\"  and an indicator of whether we have transitioned to a new sequence since the last batch (bool)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many batches do each style have?\n",
    "And when are their transitions throughout the batched data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style='display: inline-block';><tbody><tr><td>childlike<td>1</td></tr><tr><td>depressed<td>117</td></tr><tr><td>neutral<td>215</td></tr><tr><td>old<td>327</td></tr><tr><td>proud<td>426</td></tr><tr><td>sexy<td>505</td></tr><tr><td>strutting<td>642</td></tr></tbody></table>"
      ],
      "text/plain": [
       "HTML{String}(\"<table style='display: inline-block';><tbody><tr><td>childlike<td>1</td></tr><tr><td>depressed<td>117</td></tr><tr><td>neutral<td>215</td></tr><tr><td>old<td>327</td></tr><tr><td>proud<td>426</td></tr><tr><td>sexy<td>505</td></tr><tr><td>strutting<td>642</td></tr></tbody></table>\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_lens = [length(mocaputil.DataIterator(expmtdata.get_data(this_expmtdata, i, :train, :stl, split=[0.875,0.125]),\n",
    "            64, min_size=63)) for i in train_ixs];\n",
    "segment_lkp = [collect(i+1:j) for (i,j) in zip(vcat(0, cumsum(segment_lens[1:end-1])), cumsum(segment_lens))];\n",
    "segment_names = [\"angry\", \"childlike\", \"depressed\", \"neutral\", \"old\", \"proud\", \"sexy\", \"strutting\"][train_ixs];\n",
    "prettytbl.table(reshape(cumsum(vcat(1, segment_lens)[1:end-1]), :, 1), header_col=segment_names, dp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Now back to the task at hand\n",
    "\n",
    "## Generating animations from this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `file_offsets` describe the difference in position and rotation between the original and smoothed **path** trajectories.\n",
    "\n",
    "(To emphasise - only the path has had smoothing applied to it.) \n",
    "\n",
    "In order to make sure the raw data and the modelled data do not get out of sync (where the latter is the result of inputting the smoothed trajectory) we need to make sure the reconstruction of the modelled data takes this difference into account. Hence there's a bookkeeping headache of keeping such `file_offsets` around for each sequence.\n",
    "\n",
    "The following code ensures that the Forward Kinematics applied to the smoothed path is in sync with the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_offsets = CSV.File(\"../data/file_offsets_pathsmooth.csv\") |> Tables.matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**: we don't get data from the training/test split since these are batched into length-64 sequences, which are only ~ 2 seconds long. In order to get a longer animation we take from the `expmtdata` directly. Alternatively one could do something like the following:\n",
    "\n",
    "```julia\n",
    "# Recall \n",
    "# * :all - return the concatentation of all training/validation/test data.\n",
    "# * :stl - return all the data for the chosen style ix (in this case 7)\n",
    "alldata7 = mocapio.get_data(expmtdata, 7, :all, :stl)\n",
    "c_U = alldata7[3][:U][:,1:500]       # 3rd file, 1:500 seq ixs\n",
    "c_Y_tf = alldata7[3][:Y][:,1:500]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ix = 3  # file_ix ∈ [1,..,31]\n",
    "c_U = this_expmtdata.Us[file_ix]\n",
    "c_Y_tf = this_expmtdata.Ys[file_ix]\n",
    "c_Urecon = mocaputil.invert(standardize_U, Matrix(c_U'));\n",
    "c_Yrecon = mocaputil.invert(standardize_Y, Matrix(c_Y_tf'));\n",
    "\n",
    "seq_ixs = 10:600  # which elements of the sequence to visualize\n",
    "c_path_fk = geom.fk_path(c_Yrecon, seq_ixs, file_offsets[file_ix,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## three.js visualization\n",
    "\n",
    "\n",
    "Now to actually perform the visualization. This relies on `three.js` and `MeshCat.jl`. The library code included above is my attempt to map a collection of circles and cylinders to construct a skeletal visualization within this environment. This is both not difficult and not trivial. While the original code works ok modulo 2-3 tweaks due to changes in MeshCat's API, unfortunately the angle of the cylinders (i.e. bones) is slightly off in each frame. I think this is just a minor bug in `mocapviz`, but tracking it down is a bit of a pain, so in the interests of time, the visualization has this known defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "│ http://127.0.0.1:8700\n",
      "└ @ MeshCat /home/alexbird/.julia/packages/MeshCat/GlCMx/src/visualizer.jl:73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Process(`\u001b[4mxdg-open\u001b[24m \u001b[4mhttp://127.0.0.1:8700\u001b[24m`, ProcessExited(0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "# Open three.js visualization in browser if doesn't exist already\n",
    "!(@isdefined vis) && begin; vis = Visualizer(); open(vis); end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeshCat Visualizer with path /meshcat at http://127.0.0.1:8700"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = mocapviz.create_animation([geom.reconstruct_modelled(c_Yrecon[seq_ixs,:])], \n",
    "    \"test\"; vis=vis, linemesh=[mocapviz.yellowmesh], camera=:back, path = c_path_fk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "## Creating animations comparing model with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'forjulia' from '/home/alexbird/Documents/phd-work/pytorch-mtds-mocap/src/forjulia.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "pysys = pyimport(\"sys\")\n",
    "pytorch = pyimport(\"torch\")\n",
    "pushfirst!(PyVector(pysys.\"path\"), normpath(pwd(), \"../src\"));\n",
    "pymt = pyimport(\"forjulia\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
